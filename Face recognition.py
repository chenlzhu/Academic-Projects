# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mkTVbb0jMKSi-q61yTduF1euKWWAIEb6
"""

import keras
import tensorflow as tf
from functools import partial
from keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, AvgPool2D, Flatten, Dense, Dropout, MaxPooling2D
from keras import activations

import os

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/Face Recognition"
faces_path = os.path.join(path, "Faces")
detected_faces_path = os.path.join(path, "Detected Faces")

faces_names = os.listdir(faces_path) # for train
detected_faces_names = os.listdir(detected_faces_path) # for test

import pandas as pd
faces_names_list=[]
for a in range(len(faces_names)):
  faces_names_list.append(faces_names[a][:-9])

dict={}
for b in range(len(faces_names_list)):
  if (faces_names_list[b] in dict) == False:
    dict[faces_names_list[b]] = 1
  if (faces_names_list[b] in dict) == True:
    dict[faces_names_list[b]] = dict[faces_names_list[b]]+1

len(dict)

for key, value in list(dict.items()):
  if value < 50:
    del dict[key]

faces_names_valid = []
for c in range(len(faces_names)):
  if (faces_names[c][:-9] in dict.keys()) == True:
    faces_names_valid.append(faces_names[c])

detected_faces_names_valid = []
for d in range(len(detected_faces_names)):
  if (detected_faces_names[d][:-9] in dict.keys()) == True:
    detected_faces_names_valid.append(detected_faces_names[d])

import cv2

faces_train = []
faces_test = []
for i in range(len(faces_names_valid)):
  faces = cv2.imread(os.path.join(faces_path, faces_names_valid[i]))[:, :, ::-1]
  faces_train.append(faces)
  faces_label = faces_names_valid[i][:-9]
  faces_test.append(faces_label)

detected_faces_train = []
detected_faces_test = []
for j in range(len(detected_faces_names_valid)):
  try:
    detected_faces = cv2.imread(os.path.join(faces_path, detected_faces_names_valid[j]))[:, :, ::-1]
    detected_faces_train.append(detected_faces)
    detected_faces_label = detected_faces_names_valid[j][:-9]
    detected_faces_test.append(detected_faces_label)
  except TypeError:
    pass

len(pd.DataFrame(faces_test).groupby(0).count())

faces_scaled_train = tf.image.resize(faces_train, (100, 100))/255
detected_faces_scaled_train = tf.image.resize(detected_faces_train, (100, 100))/255

import numpy as np

def one_hot(data):
  one_hot_encoded = []
  for e in range(len(data)):
    if data[e] == "Ariel_Sharon":
      code = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Colin_Powell":
      code = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Donald_Rumsfeld":
      code = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "George_W_Bush":
      code = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Gerhard_Schroeder":
      code = [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Hugo_Chavez":
      code = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Jacques_Chirac":
      code = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Jean_Chretien":
      code = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "John_Ashcroft":
      code = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Junichiro_Koizumi":
      code = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Serena_Williams":
      code = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Tony_Blair":
      code = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
      one_hot_encoded.append(code)
    elif data[e] == "Vladimir_Putin":
      code = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
      one_hot_encoded.append(code)
  return np.array(one_hot_encoded)

one_hot_encoded = one_hot(faces_test)

len(pd.DataFrame(detected_faces_test).groupby(0).count())

def decode_one_hot(data):
  if max(data) < 0.:
    return "Fail to verified"
  max_index = np.argmax(data)
  code = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
  code.insert(max_index, 1)
  if code == [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:
    name = "Ariel_Sharon"
  elif code == [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:
    name = "Colin_Powell"
  elif code == [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]:
    name = "Donald_Rumsfeld"
  elif code == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]:
    name = "George_W_Bush"
  elif code == [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]:
    name = "Gerhard_Schroeder"
  elif code == [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]:
    name = "Hugo_Chavez"
  elif code == [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]:
    name = "Jacques_Chirac"
  elif code == [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]:
    name = "Jean_Chretien"
  elif code == [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]:
    name = "John_Ashcroft"
  elif code == [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]:
    name = "Junichiro_Koizumi"
  elif code == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]:
    name = "Serena_Williams"
  elif code == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]:
    name = "Tony_Blair"
  elif code == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]:
    name = "Vladimir_Putin"
  return name

from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.3)

datagen.fit(faces_scaled_train)

"""ResNet-34"""

CNN_layer = partial(Conv2D, kernel_size=3, strides=1,
                        padding="SAME", use_bias=False)

class ResidualUnitLayer(keras.layers.Layer):
  def __init__(self, filters, strides=1, activation="relu", **kwargs):
    super().__init__(**kwargs)
    self.activation = activations.get(activation)
    self.main_layers = [CNN_layer(filters, strides=strides),
                        BatchNormalization(),
                        activations.get(activation),
                        CNN_layer(filters),
                        BatchNormalization()]
    self.skip_layers = []
    if strides > 1:
      self.skip_layers = [CNN_layer(filters, kernel_size=1, strides=strides),
                          BatchNormalization()]


  def call(self, inputs):
    Z = inputs
    for layer in self.main_layers:
        Z = layer(Z)
    skip_Z = inputs
    for layer in self.skip_layers:
        skip_Z = layer(skip_Z)
    return self.activation(Z + skip_Z)

from keras.models import Sequential

model = Sequential()
model.add(CNN_layer(64, kernel_size=7, strides=2, input_shape=[100, 100, 3]))
model.add(BatchNormalization())
model.add(Activation("relu"))
model.add(MaxPool2D(pool_size=(3, 3), strides=2, padding="SAME"))
pre_filters = 64
for filters in [64]*6 + [128]*8 + [256]*12 + [512]*6:
  strides=1 if filters == pre_filters else 2
  model.add(ResidualUnitLayer(filters, strides=strides))
  pre_filters = filters
model.add(AvgPool2D(pool_size=(7, 7), strides=1, padding="SAME"))
model.add(Flatten())
model.add(Dense(1000, activation="relu"))
model.add(Dense(13, activation="sigmoid"))

model.compile(loss="categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(lr=0.000000001), metrics=['accuracy'])

history = model.fit_generator(datagen.flow(faces_scaled_train, one_hot_encoded, batch_size=32), steps_per_epoch=len(faces_scaled_train) / 32, epochs=10)

detected_faces_test_encoded = one_hot(detected_faces_test)

model.evaluate(detected_faces_scaled_train, detected_faces_test_encoded)

y_pred = model.predict(detected_faces_scaled_train)
y_pred[0]

y_pred[0].tolist()

for i in range(13):
  print(max(y_pred[i]), np.argmax(y_pred[i]))

decode_one_hot(y_pred[11])

detected_faces_test[11]

"""Resnet Tranfer Learning"""

# Transfer learning
input_size = keras.Input(shape=(100, 100, 3))
KerasModel=tf.keras.applications.resnet50.ResNet50(include_top=False, weights = "imagenet", input_tensor=input_size, classes=13)
model1 = Sequential()
model1.add(KerasModel)
model1.add(CNN_layer(64, kernel_size=7, strides=2, input_shape=[100, 100, 3]))
model1.add(BatchNormalization())
model1.add(Activation("relu"))
model1.add(MaxPool2D(pool_size=(3, 3), strides=2, padding="SAME"))
pre_filters = 64
for filters in [64]*4 + [128]*4:
  strides=1 if filters == pre_filters else 2
  model1.add(ResidualUnitLayer(filters, strides=strides))
  pre_filters = filters
model1.add(AvgPool2D(pool_size=(7, 7), strides=1, padding="SAME"))
model1.add(Flatten())
model1.add(Dense(256, activation="tanh"))
model1.add(BatchNormalization())
model1.add(Dropout(0.5))
model1.add(Dense(128, activation="tanh"))
model1.add(BatchNormalization())
model1.add(Dropout(0.25))
model1.add(Dense(1000, activation="relu"))
model1.add(Dense(13, activation="sigmoid"))

model1.compile(loss="categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(lr=0.000000001), metrics=['accuracy'])

history1 = model1.fit_generator(datagen.flow(faces_scaled_train, one_hot_encoded, batch_size=32), steps_per_epoch=len(faces_scaled_train) / 32, epochs=10)

model1.evaluate(detected_faces_scaled_train, detected_faces_test_encoded)

"""My Model"""

from keras.models import Sequential
input_shape = (100, 100, 3)
model2 = Sequential()
model2.add(Conv2D(64, (10, 10), activation='relu', input_shape=input_shape))
model2.add(MaxPooling2D(64, (2, 2), padding='same'))
model2.add(Conv2D(128, (7, 7), activation='relu'))
model2.add(MaxPooling2D(64, (2, 2), padding='same'))
model2.add(Conv2D(128, (4, 4), activation='relu'))
model2.add(MaxPooling2D(64, (2, 2), padding='same'))
model2.add(Conv2D(256, (4, 4), activation='relu'))
model2.add(Flatten())
model2.add(Dense(13, activation='sigmoid'))

model2.compile(loss="categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(lr=0.000000001), metrics=['accuracy'])

history2 = model2.fit_generator(datagen.flow(faces_scaled_train, one_hot_encoded, batch_size=32), steps_per_epoch=len(faces_scaled_train) / 32, epochs=10)

model2.evaluate(detected_faces_scaled_train, detected_faces_test_encoded)

"""VGG-19 Transfer Learning"""

from keras.models import Sequential
input_size = keras.Input(shape=(100, 100, 3))
VGGModel=tf.keras.applications.vgg19.VGG19(include_top=False, weights = "imagenet", input_tensor=input_size, classes=13)
model3 = Sequential()
model3.add(VGGModel)
model3.add(Flatten())
model3.add(BatchNormalization())
model3.add(Dense(256, activation="relu"))
model3.add(Dropout(0.5))
model3.add(BatchNormalization())
model3.add(Dense(128, activation="relu"))
model3.add(Dropout(0.5))
model3.add(BatchNormalization())
model3.add(Dense(64, activation="relu"))
model3.add(BatchNormalization())
model3.add(Dense(13, activation="sigmoid"))

model3.compile(loss="categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(lr=0.000000001), metrics=['accuracy'])

history3 = model3.fit_generator(datagen.flow(faces_scaled_train, one_hot_encoded, batch_size=32), steps_per_epoch=len(faces_scaled_train) / 32, epochs=10)

model3.evaluate(detected_faces_scaled_train, detected_faces_test_encoded)