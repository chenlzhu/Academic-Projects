# -*- coding: utf-8 -*-
"""NIH Chest X-rays.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15JqSKTisnYkYDtqUPE7iSlsftld1itTd
"""

# sc.stop()

from google.colab import drive
drive.mount('/content/drive')
path = "/content/drive/MyDrive/"

!apt-get -y install openjdk-8-jre-headless
!pip install pyspark

import sys

from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.rdd import RDD
from pyspark.sql.functions import *
from pyspark.storagelevel import StorageLevel
sc = SparkContext(appName="NIHChestX-rays").getOrCreate()
spark = SparkSession.builder.getOrCreate()

# pathCSV = sys.argv[1]
pathCSV = path+"NIHChestX-rays/Data_Entry_2017.csv"
dataEntry = sc.textFile(pathCSV)
header = sc.parallelize([dataEntry.first()])
dataEntry = dataEntry.subtract(header)
nameLabel = dataEntry.map(lambda x: x.split(',')).map(lambda x: (x[0], x[1]))

from zipfile import ZipFile
from io import BytesIO
from PIL import Image
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import OneHotEncoder

# pathImgFile = sys.argv[2]
pathImgFile = path+"NIHChestX-rays/images_001.zip"
z = ZipFile(pathImgFile)
RDDList = []
for f in z.namelist():
  img = Image.open(BytesIO(z.read(f)))
  img_cv2 = cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)
  img_resize = cv2.resize(img_cv2, (28, 28)).flatten().reshape(1, -1)
  RDDList.append((f[-16:], img_resize))
  
imgNameArray = sc.parallelize(RDDList)
arrayLabel = imgNameArray.join(nameLabel).map(lambda x: x[1])
labelArray = arrayLabel.map(lambda x: (x[1], 1))
labelCount = labelArray.reduceByKey(lambda x, y: x+y).filter(lambda x: x[1]>=100)
schema = StructType().add("Label","string").add("count","integer")
df = spark.createDataFrame(labelCount, schema=schema)
labelIndex = StringIndexer(inputCol="Label", outputCol="labelIndex")
df2 = labelIndex.fit(df).transform(df)
labelEncoding = OneHotEncoder(inputCol="labelIndex", outputCol="labelEncoding").setDropLast(False)
df3 = labelEncoding.fit(df2).transform(df2)
print(df3.show())

RDDOnehot = df3.rdd.map(lambda x: (x[0], np.array(x[3])))
labelOnehot = RDDOnehot.join(labelCount).map(lambda x: (x[0], x[1][0]))
labelArray = arrayLabel.map(lambda x: (x[1], x[0]))
labelArrayOnehot = labelArray.join(labelOnehot)
ArrayOnehot = labelArrayOnehot.map(lambda x: (x[1][0], x[1][1]))

(trainArrayOnehot1, testArrayOnehot1) = ArrayOnehot.randomSplit([0.7, 0.3])
(trainArrayOnehot2, testArrayOnehot2) = ArrayOnehot.randomSplit([0.7, 0.3])

# Neural Network with one hidden layer

def sigmoid(X):
    return 1 / (1 + np.exp(-X))

def sigmoid_prime(x):
    sig = sigmoid(x)
    return sig * (1 - sig)

def sse(y_pred, y_true):
    return 0.5 * np.sum(np.power(y_pred - y_true, 2))
  
def preforward(x, w, b):
    return np.dot(x, w) + b

def derivativeB2(y_pred, y_true, y_h, f_prime):
    return (y_pred - y_true) * f_prime(y_h)

def derivativeW2(h, dB2):
    return np.dot(h.T, dB2)

def derivativeB1(h_h, dB2, W2, f_prime):
    return np.dot(dB2, W2.T) * f_prime(h_h)

def derivativeW1(x, dB1):
    return np.dot(x.T, dB1)


num_iteration = 50
learningRate = 0.00001

input_layer = 2352 
hidden_layer = 256
output_layer = labelCount.count()


W1 = np.random.rand(input_layer, hidden_layer) - 0.5 # Shape (2352, 256)
W2 = np.random.rand(hidden_layer, output_layer) - 0.5 # Shape (256, labelCount.count())
B1 = np.random.rand(1, hidden_layer) - 0.5 # Shape (1, 256)
B2 = np.random.rand(1, output_layer) - 0.5 # Shape (1, labelCount.count())


cost_history = []
acc_history = []

for i in range(num_iteration):
    gradientCostAcc1 = trainArrayOnehot1\
        .map(lambda x: (x[0], preforward(x[0], W1, B1), x[1]))\
        .map(lambda x: (x[0], x[1], sigmoid(x[1]), x[2]))\
        .map(lambda x: (x[0], x[1], x[2], preforward(x[2], W2, B2), x[3]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], sigmoid(x[3]), x[4]))\
        .map(lambda x: (x[0], x[1], x[2], sse(x[4], x[5]), derivativeB2(x[4], x[5], x[3], sigmoid_prime), int(np.argmax(x[4]) == np.argmax(x[5]))))\
        .map(lambda x: (x[0], x[1], x[3], x[4],  derivativeW2(x[2], x[4]) ,x[5]))\
        .map(lambda x: (x[0], x[2], x[3], x[4],  derivativeB1(x[1],  x[3], W2, sigmoid_prime) ,x[5]))\
        .map(lambda x: (x[1], x[2], x[3], x[4], derivativeW1(x[0], x[4]) ,x[5], 1)) \
        .reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3], x[4] + y[4], x[5] + y[5], x[6] + y[6]))

    n = gradientCostAcc1[-1] 
    cost = gradientCostAcc1[0]/n 
    acc = gradientCostAcc1[5]/n 
    
    cost_history.append(cost)
    acc_history.append(acc)

    DB2 = gradientCostAcc1[1]/n
    DW2 = gradientCostAcc1[2]/n
    DB1 = gradientCostAcc1[3]/n
    DW1 = gradientCostAcc1[4]/n
            
    B2 -= learningRate * DB2
    W2 -= learningRate * DW2
    B1 -= learningRate * DB1
    W1 -= learningRate * DW1

    print(f"   Epoch {i+1}/{num_iteration} | Cost: {cost_history[i]} | Acc: {acc_history[i]*100} | Batchsize:{n}")

from sklearn.metrics import multilabel_confusion_matrix

def predict(x, W1, B1, W2, B2):
    return sigmoid(preforward(sigmoid(preforward(x , W1, B1)), W2, B2))

def get_metrics(pred, true):
    cm = multilabel_confusion_matrix(true, pred)
    return (cm)

def print_performance(df, metrics):
    for index, label_metrics in enumerate(metrics):
    
      label = df.filter(df.labelIndex == index).select("Label").first()[0]

      print(f"\n---- Label: {label} ------\n")
      tn, fp, fn, tp = label_metrics.ravel()
      print("TP:", tp, "FP:", fp, "FN:", fn, "TN:", tn)

      precision = tp / (tp + fp + 0.000001)
      print(f"\nPrecision : {precision}")

      recall = tp / (tp + fn + 0.000001)
      print(f"Recall: {recall}")

      F1 = 2 * (precision * recall) / (precision + recall + 0.000001)
      print(f"F1 score: {F1}")

  
metrics = testArrayOnehot1.map(lambda x: get_metrics(np.round(predict(x[0], W1, B1, W2, B2)), x[1].reshape(1, -1)))\
                  .reduce(lambda x, y: x + y)

print_performance(df3, metrics)

# Neural Network with 3 hidden layers

def sigmoid(X):
    return 1 / (1 + np.exp(-X))

def sigmoid_prime(x):
    sig = sigmoid(x)
    return sig * (1 - sig)

def sse(y_pred, y_true):
    return 0.5 * np.sum(np.power(y_pred - y_true, 2))
  
def preforward(x, w, b):
    return np.dot(x, w) + b

def predict(x, W1, B1, W2, B2, W3, B3, W4, B4):
    return sigmoid(preforward(sigmoid(preforward(sigmoid(preforward(x , W1, B1)), W2, B2), W3, B3), W4, B4))

def derivativeB4(y_pred, y_true, y_h, f_prime):
    return (y_pred - y_true) * f_prime(y_h)

def derivativeW4(h, dB4):
    return np.dot(h.T, dB4)

def derivativeB3(h_h3, dB4, W4, f_prime):
    return np.dot(dB4, W4.T) * f_prime(h_h3)

def derivativeW3(h, dB3):
    return np.dot(h.T, dB3)

def derivativeB2(h_h2, dB3, W3, f_prime):
    return np.dot(dB3, W3.T) * f_prime(h_h2)

def derivativeW2(h, dB2):
    return np.dot(h.T, dB2)

def derivativeB1(h_h1, dB2, W2, f_prime):
    return np.dot(dB2, W2.T) * f_prime(h_h1)

def derivativeW1(x, dB1):
    return np.dot(x.T, dB1)


num_iteration = 50
learningRate = 0.1

input_layer = 2352 
hidden_layer1 = 1024
hidden_layer2 = 512
hidden_layer3 = 256
output_layer = labelCount.count()

W1 = np.random.rand(input_layer, hidden_layer1) - 0.5 # Shape (2352, 1024)
W2 = np.random.rand(hidden_layer1, hidden_layer2) - 0.5 # Shape (1024, 512)
W3 = np.random.rand(hidden_layer2, hidden_layer3) - 0.5 # Shape (512, 256)
W4 = np.random.rand(hidden_layer3, output_layer) - 0.5 # Shape (256, labelCount.count())
B1 = np.random.rand(1, hidden_layer1) - 0.5 # Shape (1, 1024)
B2 = np.random.rand(1, hidden_layer2) - 0.5 # Shape (1, 512)
B3 = np.random.rand(1, hidden_layer3) - 0.5 # Shape (1, 256)
B4 = np.random.rand(1, output_layer) - 0.5 # Shape (1, labelCount.count())


cost_history, acc_history = [], []

for i in range(num_iteration):
    gradientCostAcc2 = trainArrayOnehot2\
        .map(lambda x: (x[0], preforward(x[0], W1, B1), x[1]))\
        .map(lambda x: (x[0], x[1], sigmoid(x[1]), x[2]))\
        .map(lambda x: (x[0], x[1], x[2], preforward(x[2], W2, B2), x[3]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], sigmoid(x[3]), x[4]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], preforward(x[4], W3, B3), x[5]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], sigmoid(x[5]), x[6]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], preforward(x[6], W4, B4), x[7]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], sigmoid(x[7]), x[8]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], sse(x[8], x[9]), derivativeB4(x[8], x[9], x[7], sigmoid_prime), int(np.argmax(x[8]) == np.argmax(x[9]))))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[7], x[8], derivativeW4(x[6], x[8]) ,x[9]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[6], x[7], x[8], derivativeB3(x[5],  x[7], W4, sigmoid_prime) ,x[9]))\
        .map(lambda x: (x[0], x[1], x[2], x[3], x[5], x[6], x[7], x[8], derivativeW3(x[4], x[8]) ,x[9]))\
        .map(lambda x: (x[0], x[1], x[2], x[4], x[5], x[6], x[7], x[8], derivativeB2(x[3],  x[7], W3, sigmoid_prime) ,x[9]))\
        .map(lambda x: (x[0], x[1], x[3], x[4], x[5], x[6], x[7], x[8], derivativeW2(x[2], x[8]) ,x[9]))\
        .map(lambda x: (x[0], x[2], x[3], x[4], x[5], x[6], x[7], x[8], derivativeB1(x[1],  x[7], W2, sigmoid_prime) ,x[9]))\
        .map(lambda x: (x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], derivativeW1(x[0], x[8]) ,x[9], 1)) \
        .reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3], x[4] + y[4], x[5] + y[5], x[6] + y[6], x[7] + y[7], x[8] + y[8], x[9] + y[9], x[10] + y[10]))
      
    n = gradientCostAcc2[-1] 
    cost = gradientCostAcc2[0]/n 
    acc = gradientCostAcc2[9]/n 
    
    cost_history.append(cost)
    acc_history.append(acc)

    DB4 = gradientCostAcc2[1]/n
    DW4 = gradientCostAcc2[2]/n
    DB3 = gradientCostAcc2[3]/n
    DW3 = gradientCostAcc2[4]/n
    DB2 = gradientCostAcc2[5]/n
    DW2 = gradientCostAcc2[6]/n
    DB1 = gradientCostAcc2[7]/n
    DW1 = gradientCostAcc2[8]/n
            
    B4 -= learningRate * DB4
    W4 -= learningRate * DW4
    B3 -= learningRate * DB3
    W3 -= learningRate * DW3
    B2 -= learningRate * DB2
    W2 -= learningRate * DW2
    B1 -= learningRate * DB1
    W1 -= learningRate * DW1

    print(f"   Epoch {i+1}/{num_iteration} | Cost: {cost_history[i]} | Acc: {acc_history[i]*100} | Batchsize:{n}")

def predict(x, W1, B1, W2, B2, W3, B3, W4, B4):
    return sigmoid(preforward(sigmoid(preforward(sigmoid(preforward(sigmoid(preforward(x , W1, B1)), W2, B2)), W3, B3)), W4, B4))

metrics = testArrayOnehot2.map(lambda x: get_metrics(np.round(predict(x[0], W1, B1, W2, B2, W3, B3, W4, B4)), x[1].reshape(1, -1)))\
                  .reduce(lambda x, y: x + y)

print_performance(df3, metrics)

sc.stop()
